#!/bin/bash
#SBATCH -N 1
#SBATCH -n 192
#SBATCH --partition=cpu_benchmark
#SBATCH --reservation=cpu_benchmark
#SBATCH -J HPL-CPU-AMD
#SBATCH --account=invmultifis
#SBATCH --time=24:00:00

gigaPerCore=1

module load amd-compilers/5.0.0
module load amd-libraries/5.0.0
module load amd-uprof/5.0

workdir=/scratch/invmultifis/roberto.souto6
version=v0.23.1
spackdir=${workdir}/spack/${version}
. ${spackdir}/share/spack/setup-env.sh

export SPACK_USER_CONFIG_PATH=${workdir}/.spack/${version}
#export SPACK_USER_CACHE_PATH=${SPACK_USER_CONFIG_PATH}/tmp
#export TMP=${SPACK_USER_CACHE_PATH}
#export TMPDIR=${SPACK_USER_CACHE_PATH}

spack env activate hpl_cpu_benchmark_openmpi5
spack env status

spack load openmpi@5.0.5
executable=$(spack location -i hpl)/bin/xhpl
#spack load hpl
#executable=xhpl

export ROOT=/scratch/invmultifis/roberto.souto6/benchmarks/lncc/hpl_cpu_amd

#CONT="${ROOT}/hpl-ai.sif"

echo "cmd: ${@}"

export NB=${1:-1024}
tpn=$((SLURM_NTASKS / SLURM_JOB_NUM_NODES))
gpt=$((gigaPerCore * $SLURM_CPUS_PER_TASK))
GIG_PER_TASK=${2:-${gpt}}
#GIG_PER_TASK=${2:-1}
echo "GIG PER TASK = $GIG_PER_TASK"
echo "NB = " $NB

export p=$($ROOT/utils/hpl_util.py   --tasks=$SLURM_NTASKS --compute=P )
export q=$($ROOT/utils/hpl_util.py   --tasks=$SLURM_NTASKS --compute=Q )
export P=${3:-$p}
export Q=${4:-$q}

echo "P: ${P} --- Q: ${Q}"

export N=$($ROOT/utils/hpl_util.py  --compute=N --nb=$NB --tasks=$SLURM_NTASKS --mem-per-task=$GIG_PER_TASK --ppn=$tpn)
echo "HPL params: P=$P, Q=$Q, NB=$NB, N=$N"
WKDIR=DIR_${SLURM_NNODES}_${SLURM_NTASKS}_${SLURM_JOB_ID}_gigaPerTask-${GIG_PER_TASK}_Size-${N}
mkdir $SLURM_SUBMIT_DIR/$WKDIR
cp -f $0 $WKDIR/slurm_script
cd $WKDIR


echo "Problem Size (N): $N"

cp -f $ROOT/HPL_template.dat ./HPL.dat
sed -i "s@__N__@$N@g" HPL.dat
sed -i "s@__NB__@$NB@g" HPL.dat
sed -i "s@__P__@$P@g" HPL.dat
sed -i "s@__Q__@$Q@g" HPL.dat


# Capture system specification and OpenMP settings
#CORES_PER_L3CACHE=8
CORES_PER_L3CACHE=$SLURM_CPUS_PER_TASK
# Use 4 threads per MPI rank  - this means 2 ranks per CPU L3cache (Zen 2+) or 1 rank per L3 (Zen 1), If using an "F" part (e.g 75F3) also ensure that OMP_NUM_THREADS is set appropriately
# (Recommended OMP_NUM_THREADS= #cores per L3 cache)
#export OMP_NUM_THREADS=4
export OMP_NUM_THREADS=$SLURM_CPUS_PER_TASK
echo "OMP_NUM_THREADS: $OMP_NUM_THREADS"
export OMP_PROC_BIND=TRUE  # bind threads to specific resources
export OMP_PLACES="cores"   # bind threads to cores

# AMDBlis (BLAS layer) optimizations
export BLIS_JC_NT=1  # (No outer loop parallelization)
export BLIS_IC_NT=$OMP_NUM_THREADS # (# of 2nd level threads – one per core in the shared L3 cache domain):
export BLIS_JR_NT=1 # (No 4th level threads)
export BLIS_IR_NT=1 # (No 5th level threads)

# MPI settings
MPI_RANKS=$SLURM_NTASKS
RANKS_PER_L3CACHE=$(( CORES_PER_L3CACHE / OMP_NUM_THREADS ))
echo "RANKS_PER_L3CACHE: $RANKS_PER_L3CACHE"
MPI_OPTS=”-np $MPI_RANKS --bind-to core --map-by ppr:$RANKS_PER_L3CACHE:l3cache:pe=$OMP_NUM_THREADS ”
echo "MPI_OPTS: $MPI_OPTS"

#https://clustershell.readthedocs.io/en/latest/tools/nodeset.html
nodeset -O '%s  slots='$tpn -e -S '\n' $SLURM_NODELIST 2>&1 | tee ${SLURM_SUBMIT_DIR}/hosfile-${SLURM_JOBID}.txt

echo "executable: ${executable}"
export PMIX_MCA_psec=^munge
mpirun --mca prte_silence_shared_fs 1 \
       --hostfile ${SLURM_SUBMIT_DIR}/hosfile-${SLURM_JOBID}.txt \
       $MPI_OPTS \
       ${executable}


mv ${SLURM_SUBMIT_DIR}/hosfile-${SLURM_JOBID}.txt ${SLURM_SUBMIT_DIR}/$WKDIR/
cp ${SLURM_SUBMIT_DIR}/slurm-${SLURM_JOBID}.out ${SLURM_SUBMIT_DIR}/$WKDIR/
