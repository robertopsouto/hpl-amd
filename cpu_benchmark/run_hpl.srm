#!/bin/bash
#SBATCH -N 1
#SBATCH -n 24
#SBATCH -c 8
#SBATCH --partition=cpu_benchmark
#SBATCH --reservation=cpu_benchmark
#SBATCH -J HPL-CPU-AMD
#SBATCH --account=invmultifis
#SBATCH --time=02:00:00

module load amd-compilers/5.0.0
module load amd-libraries/5.0.0
module load amd-uprof/5.0

workdir=/petrobr/parceirosbr/invmultifis
version=v0.23.1
spackdir=${workdir}/spack/${version}
. ${spackdir}/share/spack/setup-env.sh

export SPACK_USER_CONFIG_PATH=${workdir}/.spack/${version}

spack env activate hpl_gpu_benchmark
spack env status


spack load openmpi@5.0.5
spack load hpl@2.3
#executable=$(spack location -i hpl)/bin/xhpl

### Performance settings ###
# System level tunings
#echo 3 > /proc/sys/vm/drop_caches   # Clear caches to maximize available RAM
#echo 1 > /proc/sys/vm/compact_memory # Rearrange RAM usage to maximise the size of free blocks
#echo 0 > /proc/sys/kernel/numa_balancing # Prevent kernel from migrating threads overzealously
#echo 'always' > /sys/kernel/mm/transparent_hugepage/enabled # Enable hugepages for better TLB usage
#echo 'always' > /sys/kernel/mm/transparent_hugepage/defrag  # Enable page defragmentation and coalescing

# Capture system specification and OpenMP settings
#CORES_PER_L3CACHE=$SLURM_CPUS_PER_TASK
CORES_PER_L3CACHE=8
NUM_CORES=192
# Use 4 threads per MPI rank  - this means 2 ranks per CPU L3cache (Zen 2+) or 1 rank per L3 (Zen 1), If using an "F" part (e.g 75F3) also ensure that OMP_NUM_THREADS is set appropriately
# (Recommended OMP_NUM_THREADS= #cores per L3 cache)
if [ $SLURM_CPUS_PER_TASK -eq 1 ]
then
   export OMP_NUM_THREADS=1
else
   export OMP_NUM_THREADS=$((CORES_PER_L3CACHE / 2))
fi

echo "OMP_NUM_THREADS: $OMP_NUM_THREADS"

export OMP_PROC_BIND=TRUE  # bind threads to specific resources
export OMP_PLACES="cores"   # bind threads to cores

# AMDBlis (BLAS layer) optimizations
export BLIS_JC_NT=1  # (No outer loop parallelization)
export BLIS_IC_NT=$OMP_NUM_THREADS # (# of 2nd level threads â€“ one per core in the shared L3 cache domain):
export BLIS_JR_NT=1 # (No 4th level threads)
export BLIS_IR_NT=1 # (No 5th level threads)

# MPI settings
#MPI_RANKS=$(( $NUM_CORES / $OMP_NUM_THREADS ))
MPI_RANKS=$SLURM_NTASKS
RANKS_PER_L3CACHE=$(( $CORES_PER_L3CACHE / $OMP_NUM_THREADS ))
echo "RANKS_PER_L3CACHE: $RANKS_PER_L3CACHE"
MPI_OPTS="-np $MPI_RANKS --bind-to core --map-by ppr:$RANKS_PER_L3CACHE:l3cache:pe=$OMP_NUM_THREADS"
echo $MPI_OPTS


./nodeset -e $SLURM_JOB_NODELIST
echo "SLURM_JOB_NODELIST: $SLURM_JOB_NODELIST"


# Running HPL
#mpirun $MPI_OPTS xhpl

#srun -n $MPI_RANKS xhpl
